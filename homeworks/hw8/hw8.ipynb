{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание - обучить модель классификации букв для задачи расстановки ударения с помощью методов из библиотеки transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T21:41:47.281690Z",
     "iopub.status.busy": "2024-11-20T21:41:47.281335Z",
     "iopub.status.idle": "2024-11-20T21:41:48.893021Z",
     "shell.execute_reply": "2024-11-20T21:41:48.891889Z",
     "shell.execute_reply.started": "2024-11-20T21:41:47.281652Z"
    },
    "id": "5HmwpqzwnMyE",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'character-tokenizer'...\n",
      "remote: Enumerating objects: 20, done.\u001b[K\n",
      "remote: Counting objects: 100% (20/20), done.\u001b[K\n",
      "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
      "remote: Total 20 (delta 5), reused 10 (delta 3), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (20/20), 5.89 KiB | 2.95 MiB/s, done.\n",
      "Resolving deltas: 100% (5/5), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/KuzmaKhrabrov/character-tokenizer.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T21:41:48.895795Z",
     "iopub.status.busy": "2024-11-20T21:41:48.895361Z",
     "iopub.status.idle": "2024-11-20T21:41:48.900672Z",
     "shell.execute_reply": "2024-11-20T21:41:48.899595Z",
     "shell.execute_reply.started": "2024-11-20T21:41:48.895738Z"
    },
    "id": "hkVexA2nnRQ5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T21:41:48.902380Z",
     "iopub.status.busy": "2024-11-20T21:41:48.902039Z",
     "iopub.status.idle": "2024-11-20T21:41:49.439426Z",
     "shell.execute_reply": "2024-11-20T21:41:49.438557Z",
     "shell.execute_reply.started": "2024-11-20T21:41:48.902344Z"
    },
    "id": "5FaCG9ajnS_G",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import sys\n",
    "import os\n",
    "# char_tokenizer_dir = os.path.abspath('/kaggle/working/character-tokenizer')\n",
    "\n",
    "sys.path.append('/kaggle/working/character-tokenizer')\n",
    "from charactertokenizer import CharacterTokenizer\n",
    "\n",
    "chars = \"АаБбВвГгДдЕеЁёЖжЗзИиЙйКкЛлМмНнОоПпРрСсТтУуФфХхЦцЧчШшЩщЪъЫыЬьЭэЮюЯя\"\n",
    "model_max_length = 64\n",
    "tokenizer = CharacterTokenizer(chars, model_max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T21:41:49.442630Z",
     "iopub.status.busy": "2024-11-20T21:41:49.441927Z",
     "iopub.status.idle": "2024-11-20T21:41:49.447192Z",
     "shell.execute_reply": "2024-11-20T21:41:49.446174Z",
     "shell.execute_reply.started": "2024-11-20T21:41:49.442589Z"
    },
    "id": "I5FSPMOSncpI",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [0, 39, 42, 26, 12, 18, 46, 1], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "example = \"Привет\"\n",
    "tokens = tokenizer(example)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQkp36rEoScR"
   },
   "source": [
    "Датасет для обучения брал отсюда: https://github.com/Koziev/NLP_Datasets/blob/master/Stress/all_accents.zip\n",
    "\n",
    "Задание:  \n",
    "\n",
    "1. Напишите класс для Dataset/Dataloder и разбейте данные на случайные train / test сплиты в соотношении 50:50.\n",
    "2. Попробуйте обучить одну или несколько из моделей: Bert, Albert, Deberta. Посчитайте метрику Accuracy на train и test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T21:41:49.448537Z",
     "iopub.status.busy": "2024-11-20T21:41:49.448229Z",
     "iopub.status.idle": "2024-11-20T21:41:49.460523Z",
     "shell.execute_reply": "2024-11-20T21:41:49.459719Z",
     "shell.execute_reply.started": "2024-11-20T21:41:49.448502Z"
    },
    "id": "mRVK6TNAZQFk",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 72, 60, 26, 30, 38, 38, 10, 42, 6, 8, 24, 36, 64, 28, 1], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('ящикообр^азный')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T21:41:49.462022Z",
     "iopub.status.busy": "2024-11-20T21:41:49.461649Z",
     "iopub.status.idle": "2024-11-20T21:41:49.470509Z",
     "shell.execute_reply": "2024-11-20T21:41:49.469804Z",
     "shell.execute_reply.started": "2024-11-20T21:41:49.461986Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T21:41:49.471798Z",
     "iopub.status.busy": "2024-11-20T21:41:49.471478Z",
     "iopub.status.idle": "2024-11-20T21:41:52.061172Z",
     "shell.execute_reply": "2024-11-20T21:41:52.060135Z",
     "shell.execute_reply.started": "2024-11-20T21:41:49.471740Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>words_with_accent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-де</td>\n",
       "      <td>-д^е</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-ка</td>\n",
       "      <td>-к^а</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-либо</td>\n",
       "      <td>-л^ибо</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-нибудь</td>\n",
       "      <td>-ниб^удь</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-с</td>\n",
       "      <td>-с</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1680530</th>\n",
       "      <td>ӂюль-верновский</td>\n",
       "      <td>ӂюль-в^ерновский</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1680531</th>\n",
       "      <td>ӂюрить</td>\n",
       "      <td>ӂюр^ить</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1680532</th>\n",
       "      <td>ӂӂение</td>\n",
       "      <td>ӂӂ^ение</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1680533</th>\n",
       "      <td>ӂӂенный</td>\n",
       "      <td>ӂӂенный</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1680534</th>\n",
       "      <td>ӂӂеный</td>\n",
       "      <td>ӂӂеный</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1680535 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   words words_with_accent\n",
       "0                    -де              -д^е\n",
       "1                    -ка              -к^а\n",
       "2                  -либо            -л^ибо\n",
       "3                -нибудь          -ниб^удь\n",
       "4                     -с                -с\n",
       "...                  ...               ...\n",
       "1680530  ӂюль-верновский  ӂюль-в^ерновский\n",
       "1680531           ӂюрить           ӂюр^ить\n",
       "1680532           ӂӂение           ӂӂ^ение\n",
       "1680533          ӂӂенный           ӂӂенный\n",
       "1680534           ӂӂеный            ӂӂеный\n",
       "\n",
       "[1680535 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('/kaggle/input/word-stress-dataset/all_accents.tsv', delimiter = '\\t',names = ['words','words_with_accent'])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T21:41:52.062248Z",
     "iopub.status.busy": "2024-11-20T21:41:52.062014Z",
     "iopub.status.idle": "2024-11-20T21:41:52.363142Z",
     "shell.execute_reply": "2024-11-20T21:41:52.362299Z",
     "shell.execute_reply.started": "2024-11-20T21:41:52.062226Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(words                1680535\n",
       " words_with_accent    1680535\n",
       " dtype: int64,\n",
       " words                0\n",
       " words_with_accent    0\n",
       " dtype: int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count(), data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создадим датасет и даталоадер"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала найдем фразу максимальной длины -> трансформер может работать с эмбеддингами одной длины"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T21:41:52.364850Z",
     "iopub.status.busy": "2024-11-20T21:41:52.364251Z",
     "iopub.status.idle": "2024-11-20T21:43:07.306288Z",
     "shell.execute_reply": "2024-11-20T21:43:07.305320Z",
     "shell.execute_reply.started": "2024-11-20T21:41:52.364810Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1680535/1680535 [01:14<00:00, 22428.07it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(58, 'лланвайрпуллгуингиллгогерихуирндробуллллантисилиогогогох')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 0\n",
    "wordmax =''\n",
    "for word in tqdm(data['words']):\n",
    "    emb = tokenizer.encode(word, add_special_tokens=True)\n",
    "    if len(word) > max_len:\n",
    "        \n",
    "        max_len = len(emb)\n",
    "        wordmax = word\n",
    "\n",
    "max_len, wordmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T21:43:07.310072Z",
     "iopub.status.busy": "2024-11-20T21:43:07.309811Z",
     "iopub.status.idle": "2024-11-20T21:47:07.056465Z",
     "shell.execute_reply": "2024-11-20T21:47:07.055572Z",
     "shell.execute_reply.started": "2024-11-20T21:43:07.310046Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2837: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "1680535it [03:59, 7010.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  6, 16, 18,  1,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          4,  4,  4,  4]])\n",
      "tensor([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "input_ids = []\n",
    "attention_masks = []\n",
    "labels = []\n",
    "\n",
    "for i, word in tqdm(enumerate(data['words'])):\n",
    "    \n",
    "    \n",
    "    labels.append( len(data['words_with_accent'][i].split('^')[0]) )\n",
    "    \n",
    "    encoded_word = tokenizer.encode_plus(word,\n",
    "                         add_special_tokens=True,\n",
    "                         max_length=max_len,\n",
    "                         pad_to_max_length=True,\n",
    "                         return_attention_mask = True,\n",
    "                         return_tensors = 'pt' \n",
    "                         )\n",
    "\n",
    "    input_ids.append(encoded_word['input_ids'])\n",
    "    attention_masks.append(encoded_word['attention_mask'])\n",
    "    \n",
    "   \n",
    "\n",
    "print(input_ids[0])\n",
    "print(attention_masks[0])\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T21:47:07.057719Z",
     "iopub.status.busy": "2024-11-20T21:47:07.057473Z",
     "iopub.status.idle": "2024-11-20T21:47:07.063924Z",
     "shell.execute_reply": "2024-11-20T21:47:07.063022Z",
     "shell.execute_reply.started": "2024-11-20T21:47:07.057695Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  6, 16, 18,  1,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          4,  4,  4,  4]])\n",
      "tensor([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(input_ids[0])\n",
    "print(attention_masks[0])\n",
    "print(labels[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T21:47:07.065056Z",
     "iopub.status.busy": "2024-11-20T21:47:07.064734Z",
     "iopub.status.idle": "2024-11-20T21:47:16.110981Z",
     "shell.execute_reply": "2024-11-20T21:47:16.110021Z",
     "shell.execute_reply.started": "2024-11-20T21:47:07.065031Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T21:47:16.112461Z",
     "iopub.status.busy": "2024-11-20T21:47:16.112187Z",
     "iopub.status.idle": "2024-11-20T21:47:16.249012Z",
     "shell.execute_reply": "2024-11-20T21:47:16.248051Z",
     "shell.execute_reply.started": "2024-11-20T21:47:16.112435Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split, TensorDataset, DataLoader\n",
    "\n",
    "dataset = TensorDataset(input_ids,attention_masks, labels)\n",
    "\n",
    "train_size = int(0.5 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T21:47:16.250604Z",
     "iopub.status.busy": "2024-11-20T21:47:16.250209Z",
     "iopub.status.idle": "2024-11-20T21:47:16.256214Z",
     "shell.execute_reply": "2024-11-20T21:47:16.255418Z",
     "shell.execute_reply.started": "2024-11-20T21:47:16.250568Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            sampler = RandomSampler(train_dataset), \n",
    "            batch_size = batch_size \n",
    "        )\n",
    "\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "            test_dataset,\n",
    "            sampler = SequentialSampler(test_dataset), \n",
    "            batch_size = batch_size \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T21:47:16.257669Z",
     "iopub.status.busy": "2024-11-20T21:47:16.257337Z",
     "iopub.status.idle": "2024-11-20T21:47:16.300136Z",
     "shell.execute_reply": "2024-11-20T21:47:16.299340Z",
     "shell.execute_reply.started": "2024-11-20T21:47:16.257633Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T21:47:16.301402Z",
     "iopub.status.busy": "2024-11-20T21:47:16.301082Z",
     "iopub.status.idle": "2024-11-20T21:47:21.650323Z",
     "shell.execute_reply": "2024-11-20T21:47:21.649624Z",
     "shell.execute_reply.started": "2024-11-20T21:47:16.301376Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23e8d9418e734efe936b7d1d0d0a8e76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fd37c350c2d45dc838bfd9b67d13f3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/672M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertConfig\n",
    "\n",
    "\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single\n",
    "# linear classification layer on top.\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-multilingual-uncased\", \n",
    "    num_labels = max_len, \n",
    "                   \n",
    "    output_attentions = False, \n",
    "    output_hidden_states = False, \n",
    ")\n",
    "\n",
    "\n",
    "if device == \"cuda\":\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T21:47:21.651841Z",
     "iopub.status.busy": "2024-11-20T21:47:21.651408Z",
     "iopub.status.idle": "2024-11-20T21:47:22.061600Z",
     "shell.execute_reply": "2024-11-20T21:47:22.060730Z",
     "shell.execute_reply.started": "2024-11-20T21:47:21.651812Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 1e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "epochs = 4\n",
    "\n",
    "\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps = 0,\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функции для подсчета времени выполнения и метрики accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T21:47:22.063727Z",
     "iopub.status.busy": "2024-11-20T21:47:22.062929Z",
     "iopub.status.idle": "2024-11-20T21:47:22.068314Z",
     "shell.execute_reply": "2024-11-20T21:47:22.067463Z",
     "shell.execute_reply.started": "2024-11-20T21:47:22.063685Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T21:47:22.070308Z",
     "iopub.status.busy": "2024-11-20T21:47:22.069444Z",
     "iopub.status.idle": "2024-11-20T21:47:22.080813Z",
     "shell.execute_reply": "2024-11-20T21:47:22.080138Z",
     "shell.execute_reply.started": "2024-11-20T21:47:22.070268Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T21:47:22.093284Z",
     "iopub.status.busy": "2024-11-20T21:47:22.092883Z",
     "iopub.status.idle": "2024-11-21T04:40:42.801808Z",
     "shell.execute_reply": "2024-11-21T04:40:42.800966Z",
     "shell.execute_reply.started": "2024-11-20T21:47:22.093249Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch   700  of  26,259.    Elapsed: 0:02:09.\n",
      "  Batch 1,400  of  26,259.    Elapsed: 0:04:18.\n",
      "  Batch 2,100  of  26,259.    Elapsed: 0:06:27.\n",
      "  Batch 2,800  of  26,259.    Elapsed: 0:08:36.\n",
      "  Batch 3,500  of  26,259.    Elapsed: 0:10:45.\n",
      "  Batch 4,200  of  26,259.    Elapsed: 0:12:54.\n",
      "  Batch 4,900  of  26,259.    Elapsed: 0:15:03.\n",
      "  Batch 5,600  of  26,259.    Elapsed: 0:17:12.\n",
      "  Batch 6,300  of  26,259.    Elapsed: 0:19:21.\n",
      "  Batch 7,000  of  26,259.    Elapsed: 0:21:30.\n",
      "  Batch 7,700  of  26,259.    Elapsed: 0:23:39.\n",
      "  Batch 8,400  of  26,259.    Elapsed: 0:25:48.\n",
      "  Batch 9,100  of  26,259.    Elapsed: 0:27:57.\n",
      "  Batch 9,800  of  26,259.    Elapsed: 0:30:05.\n",
      "  Batch 10,500  of  26,259.    Elapsed: 0:32:14.\n",
      "  Batch 11,200  of  26,259.    Elapsed: 0:34:23.\n",
      "  Batch 11,900  of  26,259.    Elapsed: 0:36:32.\n",
      "  Batch 12,600  of  26,259.    Elapsed: 0:38:41.\n",
      "  Batch 13,300  of  26,259.    Elapsed: 0:40:50.\n",
      "  Batch 14,000  of  26,259.    Elapsed: 0:42:59.\n",
      "  Batch 14,700  of  26,259.    Elapsed: 0:45:07.\n",
      "  Batch 15,400  of  26,259.    Elapsed: 0:47:16.\n",
      "  Batch 16,100  of  26,259.    Elapsed: 0:49:25.\n",
      "  Batch 16,800  of  26,259.    Elapsed: 0:51:34.\n",
      "  Batch 17,500  of  26,259.    Elapsed: 0:53:43.\n",
      "  Batch 18,200  of  26,259.    Elapsed: 0:55:52.\n",
      "  Batch 18,900  of  26,259.    Elapsed: 0:58:01.\n",
      "  Batch 19,600  of  26,259.    Elapsed: 1:00:10.\n",
      "  Batch 20,300  of  26,259.    Elapsed: 1:02:19.\n",
      "  Batch 21,000  of  26,259.    Elapsed: 1:04:29.\n",
      "  Batch 21,700  of  26,259.    Elapsed: 1:06:38.\n",
      "  Batch 22,400  of  26,259.    Elapsed: 1:08:47.\n",
      "  Batch 23,100  of  26,259.    Elapsed: 1:10:56.\n",
      "  Batch 23,800  of  26,259.    Elapsed: 1:13:05.\n",
      "  Batch 24,500  of  26,259.    Elapsed: 1:15:14.\n",
      "  Batch 25,200  of  26,259.    Elapsed: 1:17:23.\n",
      "  Batch 25,900  of  26,259.    Elapsed: 1:19:32.\n",
      "\n",
      "  Average training loss: 0.72\n",
      "  Training epoch took: 1:20:38\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.85\n",
      "  Validation Loss: 0.40\n",
      "  Validation took: 0:22:21\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch   700  of  26,259.    Elapsed: 0:02:09.\n",
      "  Batch 1,400  of  26,259.    Elapsed: 0:04:18.\n",
      "  Batch 2,100  of  26,259.    Elapsed: 0:06:27.\n",
      "  Batch 2,800  of  26,259.    Elapsed: 0:08:36.\n",
      "  Batch 3,500  of  26,259.    Elapsed: 0:10:45.\n",
      "  Batch 4,200  of  26,259.    Elapsed: 0:12:54.\n",
      "  Batch 4,900  of  26,259.    Elapsed: 0:15:03.\n",
      "  Batch 5,600  of  26,259.    Elapsed: 0:17:12.\n",
      "  Batch 6,300  of  26,259.    Elapsed: 0:19:20.\n",
      "  Batch 7,000  of  26,259.    Elapsed: 0:21:30.\n",
      "  Batch 7,700  of  26,259.    Elapsed: 0:23:38.\n",
      "  Batch 8,400  of  26,259.    Elapsed: 0:25:47.\n",
      "  Batch 9,100  of  26,259.    Elapsed: 0:27:56.\n",
      "  Batch 9,800  of  26,259.    Elapsed: 0:30:05.\n",
      "  Batch 10,500  of  26,259.    Elapsed: 0:32:14.\n",
      "  Batch 11,200  of  26,259.    Elapsed: 0:34:23.\n",
      "  Batch 11,900  of  26,259.    Elapsed: 0:36:32.\n",
      "  Batch 12,600  of  26,259.    Elapsed: 0:38:41.\n",
      "  Batch 13,300  of  26,259.    Elapsed: 0:40:50.\n",
      "  Batch 14,000  of  26,259.    Elapsed: 0:42:59.\n",
      "  Batch 14,700  of  26,259.    Elapsed: 0:45:08.\n",
      "  Batch 15,400  of  26,259.    Elapsed: 0:47:17.\n",
      "  Batch 16,100  of  26,259.    Elapsed: 0:49:26.\n",
      "  Batch 16,800  of  26,259.    Elapsed: 0:51:35.\n",
      "  Batch 17,500  of  26,259.    Elapsed: 0:53:44.\n",
      "  Batch 18,200  of  26,259.    Elapsed: 0:55:53.\n",
      "  Batch 18,900  of  26,259.    Elapsed: 0:58:02.\n",
      "  Batch 19,600  of  26,259.    Elapsed: 1:00:11.\n",
      "  Batch 20,300  of  26,259.    Elapsed: 1:02:20.\n",
      "  Batch 21,000  of  26,259.    Elapsed: 1:04:29.\n",
      "  Batch 21,700  of  26,259.    Elapsed: 1:06:38.\n",
      "  Batch 22,400  of  26,259.    Elapsed: 1:08:47.\n",
      "  Batch 23,100  of  26,259.    Elapsed: 1:10:56.\n",
      "  Batch 23,800  of  26,259.    Elapsed: 1:13:05.\n",
      "  Batch 24,500  of  26,259.    Elapsed: 1:15:13.\n",
      "  Batch 25,200  of  26,259.    Elapsed: 1:17:22.\n",
      "  Batch 25,900  of  26,259.    Elapsed: 1:19:31.\n",
      "\n",
      "  Average training loss: 0.38\n",
      "  Training epoch took: 1:20:37\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.89\n",
      "  Validation Loss: 0.28\n",
      "  Validation took: 0:22:21\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch   700  of  26,259.    Elapsed: 0:02:09.\n",
      "  Batch 1,400  of  26,259.    Elapsed: 0:04:18.\n",
      "  Batch 2,100  of  26,259.    Elapsed: 0:06:27.\n",
      "  Batch 2,800  of  26,259.    Elapsed: 0:08:36.\n",
      "  Batch 3,500  of  26,259.    Elapsed: 0:10:45.\n",
      "  Batch 4,200  of  26,259.    Elapsed: 0:12:54.\n",
      "  Batch 4,900  of  26,259.    Elapsed: 0:15:03.\n",
      "  Batch 5,600  of  26,259.    Elapsed: 0:17:12.\n",
      "  Batch 6,300  of  26,259.    Elapsed: 0:19:21.\n",
      "  Batch 7,000  of  26,259.    Elapsed: 0:21:30.\n",
      "  Batch 7,700  of  26,259.    Elapsed: 0:23:40.\n",
      "  Batch 8,400  of  26,259.    Elapsed: 0:25:50.\n",
      "  Batch 9,100  of  26,259.    Elapsed: 0:27:59.\n",
      "  Batch 9,800  of  26,259.    Elapsed: 0:30:09.\n",
      "  Batch 10,500  of  26,259.    Elapsed: 0:32:19.\n",
      "  Batch 11,200  of  26,259.    Elapsed: 0:34:29.\n",
      "  Batch 11,900  of  26,259.    Elapsed: 0:36:38.\n",
      "  Batch 12,600  of  26,259.    Elapsed: 0:38:48.\n",
      "  Batch 13,300  of  26,259.    Elapsed: 0:40:58.\n",
      "  Batch 14,000  of  26,259.    Elapsed: 0:43:07.\n",
      "  Batch 14,700  of  26,259.    Elapsed: 0:45:17.\n",
      "  Batch 15,400  of  26,259.    Elapsed: 0:47:27.\n",
      "  Batch 16,100  of  26,259.    Elapsed: 0:49:37.\n",
      "  Batch 16,800  of  26,259.    Elapsed: 0:51:46.\n",
      "  Batch 17,500  of  26,259.    Elapsed: 0:53:56.\n",
      "  Batch 18,200  of  26,259.    Elapsed: 0:56:06.\n",
      "  Batch 18,900  of  26,259.    Elapsed: 0:58:16.\n",
      "  Batch 19,600  of  26,259.    Elapsed: 1:00:25.\n",
      "  Batch 20,300  of  26,259.    Elapsed: 1:02:35.\n",
      "  Batch 21,000  of  26,259.    Elapsed: 1:04:45.\n",
      "  Batch 21,700  of  26,259.    Elapsed: 1:06:54.\n",
      "  Batch 22,400  of  26,259.    Elapsed: 1:09:04.\n",
      "  Batch 23,100  of  26,259.    Elapsed: 1:11:14.\n",
      "  Batch 23,800  of  26,259.    Elapsed: 1:13:24.\n",
      "  Batch 24,500  of  26,259.    Elapsed: 1:15:33.\n",
      "  Batch 25,200  of  26,259.    Elapsed: 1:17:43.\n",
      "  Batch 25,900  of  26,259.    Elapsed: 1:19:53.\n",
      "\n",
      "  Average training loss: 0.29\n",
      "  Training epoch took: 1:20:59\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.92\n",
      "  Validation Loss: 0.22\n",
      "  Validation took: 0:22:39\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch   700  of  26,259.    Elapsed: 0:02:10.\n",
      "  Batch 1,400  of  26,259.    Elapsed: 0:04:19.\n",
      "  Batch 2,100  of  26,259.    Elapsed: 0:06:29.\n",
      "  Batch 2,800  of  26,259.    Elapsed: 0:08:39.\n",
      "  Batch 3,500  of  26,259.    Elapsed: 0:10:49.\n",
      "  Batch 4,200  of  26,259.    Elapsed: 0:12:58.\n",
      "  Batch 4,900  of  26,259.    Elapsed: 0:15:08.\n",
      "  Batch 5,600  of  26,259.    Elapsed: 0:17:18.\n",
      "  Batch 6,300  of  26,259.    Elapsed: 0:19:28.\n",
      "  Batch 7,000  of  26,259.    Elapsed: 0:21:37.\n",
      "  Batch 7,700  of  26,259.    Elapsed: 0:23:47.\n",
      "  Batch 8,400  of  26,259.    Elapsed: 0:25:57.\n",
      "  Batch 9,100  of  26,259.    Elapsed: 0:28:07.\n",
      "  Batch 9,800  of  26,259.    Elapsed: 0:30:16.\n",
      "  Batch 10,500  of  26,259.    Elapsed: 0:32:26.\n",
      "  Batch 11,200  of  26,259.    Elapsed: 0:34:36.\n",
      "  Batch 11,900  of  26,259.    Elapsed: 0:36:45.\n",
      "  Batch 12,600  of  26,259.    Elapsed: 0:38:55.\n",
      "  Batch 13,300  of  26,259.    Elapsed: 0:41:05.\n",
      "  Batch 14,000  of  26,259.    Elapsed: 0:43:15.\n",
      "  Batch 14,700  of  26,259.    Elapsed: 0:45:24.\n",
      "  Batch 15,400  of  26,259.    Elapsed: 0:47:34.\n",
      "  Batch 16,100  of  26,259.    Elapsed: 0:49:44.\n",
      "  Batch 16,800  of  26,259.    Elapsed: 0:51:54.\n",
      "  Batch 17,500  of  26,259.    Elapsed: 0:54:03.\n",
      "  Batch 18,200  of  26,259.    Elapsed: 0:56:13.\n",
      "  Batch 18,900  of  26,259.    Elapsed: 0:58:23.\n",
      "  Batch 19,600  of  26,259.    Elapsed: 1:00:33.\n",
      "  Batch 20,300  of  26,259.    Elapsed: 1:02:42.\n",
      "  Batch 21,000  of  26,259.    Elapsed: 1:04:52.\n",
      "  Batch 21,700  of  26,259.    Elapsed: 1:07:02.\n",
      "  Batch 22,400  of  26,259.    Elapsed: 1:09:11.\n",
      "  Batch 23,100  of  26,259.    Elapsed: 1:11:21.\n",
      "  Batch 23,800  of  26,259.    Elapsed: 1:13:31.\n",
      "  Batch 24,500  of  26,259.    Elapsed: 1:15:41.\n",
      "  Batch 25,200  of  26,259.    Elapsed: 1:17:50.\n",
      "  Batch 25,900  of  26,259.    Elapsed: 1:20:00.\n",
      "\n",
      "  Average training loss: 0.25\n",
      "  Training epoch took: 1:21:07\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.93\n",
      "  Validation Loss: 0.20\n",
      "  Validation took: 0:22:39\n",
      "\n",
      "Training complete!\n",
      "Total training took 6:53:21 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "training_stats = []\n",
    "\n",
    "\n",
    "total_t0 = time.time()\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "   \n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        if step % 700 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        output = model(b_input_ids,\n",
    "                             token_type_ids=None,\n",
    "                             attention_mask=b_input_mask,\n",
    "                             labels=b_labels)\n",
    "\n",
    "        total_train_loss += output.loss.item()\n",
    "\n",
    "        output.loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epoch took: {:}\".format(training_time))\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    model.save_pretrained('./BERT_word_stress/')\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "\n",
    "        #   [0]: input ids\n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(b_input_ids,\n",
    "                                   token_type_ids=None,\n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "\n",
    "        total_eval_loss += output.loss.item()\n",
    "\n",
    "        logits = output.logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "    \n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, обучение заняло почти 7 часов на 4 эпохи. Думаю, что с моделью типа Albert обучение бы сильно ускорилось. Однако accuracy получилась отличная - 0.93 "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6131122,
     "sourceId": 9966616,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
